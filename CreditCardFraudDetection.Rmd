---
title: "HarvardX Data Science Program"
author: "Do Quang Anh"
date: "`r format(Sys.Date())`"
output:
  pdf_document: 
    number_sections: yes
    fig_caption: yes
    toc: yes
    fig_height: 3
    includes: null
    latex_engine: lualatex
    keep_tex: yes
  html_document:
    toc: yes
    df_print: paged
  word_document:
    toc: yes
subtitle: Credit Card Fraud Detection project
email: mr.anhdq@gmail.com
---

```{r Install Packages, include=FALSE}
##Installing Packages
# List of packages for session
.packages = c("tidyverse",       #tidy alvvays and forever!
              "corrplot",        #correlation plots
              "cowplot",         #solve x-axis misalignment when plotting, and better-looking defaults for ggplots
              "gridExtra",       #combine plots
              "knitr",           #report output
              "kableExtra",      #nice tables
              "lubridate",       #date math!
              "reshape2",        #acast to create matrix
              "scales",          #get rid of scientific notation in charts
              "splitstackshape",  #explode pipe-delimited data to one-hot encoded dummy variables
              "dplyr",
              "tm",
              "tmap",
              "wordcloud",
              "knitr",
              "tinytex",
              "kableExtra",
              "tidyr",
              "stringr",
              "ggplot2",
              "gbm",
              "caret",
              "xgboost",
              "e1071",
              "class",
              "ROCR",
              "randomForest",
              "PRROC",
              "reshape2",
              "caTools",
              "Rtsne",
              "data.table"
             
              )


# Install CRAN packages (if not already installed)
.inst <- .packages %in% installed.packages()
if(length(.packages[!.inst]) > 0) install.packages(.packages[!.inst])
# Load packages into session 
lapply(.packages, require, character.only=TRUE)
#tinytex::install_tinytex()

```

```{r Functions and Hooks, include=FALSE}
# Customize knitr output
#Set Thousands Separator for inline output
knitr::knit_hooks$set(inline = function(x) { if(!is.numeric(x)){ x }else{ prettyNum(round(x,2), big.mark=",") } })
#we've already set the graphic device to "png" in the RMD options. the default device for pdfs draws every point of a scatterplot, creatinvg *very* big files.
#But png is not as crisp, so we will set a higher resolution for pdf output of plots. 
knitr::opts_chunk$set(dpi=150)
#Create Kable wrapper function for thousands separator in table output, and nice formating with kableExtra
niceKable = function(...) {
  knitr::kable(..., format.args = list(decimal.mark = '.', big.mark = ",")) %>% kable_styling()
}

```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newpage

# Introduction

Billions of dollars of loss are caused every year due to fraudulent credit card transactions. The design of efficient fraud detection algorithms is key to reducing these losses, and more algorithms rely on advanced machine learning techniques to assist fraud investigators. The design of fraud detection algorithms is however particularly challenging due to non-stationary distribution of the data, highly imbalanced classes distributions and continuous streams of transactions. At the same time public data are scarcely available for confidentiality issues, leaving unanswered many questions about which is the best strategy to handle this issue.

## About Dataset

The dataset contains transactions made by credit cards in September 2013 by European cardholders.This dataset from Kaggle is available here: <https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud>. This dataset presents transactions that occurred in two days, where have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.

It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.

Given the class imbalance ratio, recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). We will also use different sampling techniques (details below) on the train dataset in order to address the issue of imbalanced classes while training our models.

\newpage

# Exploratory data analysis and data cleaning

```{r echo=FALSE}
# loading the data
if (!(file.exists('creditcard.csv'))) {
 download.file("http://ngocanhparis.online/creditcard.csv",'creditcard.csv')
}
df = read.csv('creditcard.csv')
```

```{r eda, echo=FALSE, include=TRUE}
tribble(
  ~"Dataset",     ~"Number of Rows",    ~"Number of Columns",
  #--             |--                   |----
  "Credit card",   nrow(df),            ncol(df),
  
)%>%niceKable
```
\newpage
All the features, apart from "time" and "amount" are anonymised. Let's see whether there is any missing data.

```{r CHECKMISSING, echo=FALSE, include=TRUE}
sapply(df, function(x) sum(is.na(x)))%>% niceKable

```

There are no NA values in the data.

\newpage
## Check for class imbalance

Unbalanced data refers to unequal instances of different classes. The visualization shown below further reflects the imbalance of non-fraud and fraud transactions in the dataset. We have class (0 --- No fraud, 1 --- fraud) on the X-axis and the percentage of instances plotted on Y-axis. We see that our dataset is highly unbalanced with respect to the class of interest(Fraud).

```{r Checkclassimbalance , echo=FALSE, include=TRUE}
df %>%
  group_by(Class) %>% 
  summarise(Count = n()) %>% niceKable

```

```{r exploreClassImbalance, echo=FALSE, include=TRUE, fig.height = 7 }
## Checking Class imbalance

common_theme <- theme(plot.title = element_text(hjust = 0.5, face = "bold"))
ggplot(data = df, aes(x = factor(Class), y = prop.table(stat(count)), 
                             fill = factor(Class),
                             label = scales::percent(prop.table(stat(count))))) +
  scale_fill_brewer(palette = "Set2") +
  geom_bar(position = "dodge") +
  geom_text(stat = 'count',
            position = position_dodge(.9),
            vjust = -0.5,
            size = 3) +
  scale_x_discrete(labels = c("no fraud", "fraud")) +
  scale_y_continuous(labels = scales::percent) +
  labs(x = 'Class', y = 'Percentage') + 
  ggtitle ("Distribution of Class Variable") +
  common_theme
```

\newpage

## Time variable - Frauds over Time Distribution

In the graph below, notice that the number of regular transactions drops sharply around the 90,000th-second mark, to surge again around the 110,000th-second mark. It wouldn't be absurd to assume that this period is during the night when individuals naturally perform fewer purchases and transactions than during the daytime.

On the other hand, a great number of fraudulent transactions occurred around the 100,000 mark, which could confirm the previous assumption, considering that criminals should prefer to commit fraud late at night, assuming there would be less surveillance and victims would not realize they were being scammed soon enough.

```{r exploreTime}
df %>%
  ggplot(aes(x = Time, fill = factor(Class))) + geom_histogram(bins = 100)+
  labs(x = 'Time in Seconds Since First Transaction', y = 'No. of Transactions') +
  ggtitle('Distribution of Time of Transaction by Class') +
  scale_fill_brewer(palette = "Set2") +
  facet_grid(Class ~ ., scales = 'free_y') + common_theme
```

## Amount Variable

The boxplot below demonstrates the Amount of each transaction is more variable with the non-fraud transactions than with the fraud transactions given the number of outliers. Most transactions, both regular and fraudulent, were of "small" values. Small amount of money, less or equal of one dollar are scammed more frequently.

```{r exploreAmount, echo=FALSE, include=TRUE}
ggplot(df,aes(x = factor(Class), y =  Amount)) +
  geom_boxplot()+
  labs(x= 'Class (non-Fraud vs Fraud)', y = 'Amount (Euros)') +
  ggtitle("Distribution of Transaction Amount by Class") +
  common_theme

```

```{r exploreFraudAmount, echo=FALSE, include=TRUE}
df[df$Class == 1,] %>%
  ggplot(aes(Amount))+
  geom_histogram(col = "black", fill ="darkseagreen3",binwidth = 40)+
  labs(x ='Amount', y ='Frequency')+
  ggtitle('Frauds Amounts Distributions')

```

## Correlations between each variables

```{r exploreCorrelation, echo=FALSE, include=TRUE}
#Correlations
correlations <- cor(df[,], method='spearman')
round(correlations, 2)
##title <- "Correlation of Fraud Dataset Variables"
corrplot(correlations, number.cex = .9, type = "full",
              method = "color", tl.cex=0.8,tl.col = "black")
```

\newpage

# Analysis - Models Building and Comparison

## Data Pre Processing

**1. Remove the "Time" column and Change 'Class' variable to factor from the dataset**

```{r removeTime, echo=TRUE, include=TRUE}
# Set seed for reproducibility
set.seed(1234)
# Remove the "Time" column from the dataset
df <- df %>% select(-Time)
#Change 'Class' variable to factor
df$Class <- as.factor(df$Class)
```

**2. Split the dataset into train, test, cv dataset**

```{r splitData, echo=TRUE, include=TRUE}
# Split the dataset into train, test and cross validation dataset
train_index = createDataPartition(y = df$Class,
                                  p = .6,
                                  list = F)
train <- df[train_index,]
test_cv <-df[-train_index,]

test_index = createDataPartition(y = test_cv$Class,
                                  p = .5,
                                  list = F)

test <- test_cv[test_index,]
cv <- test_cv[-test_index,]

rm(train_index, test_index, test_cv)
```

## Classification Models

Classification is the process of predicting discrete variables (1/0, Yes/no, etc.). Given the case with our dataset, it will be more optimistic to deploy a classification model rather than any others. To better understand which algorithm would perform best on the given dataset, the following algorithms are used:Naive Bayes, KNN, Random Forest,XGBoost

### Naive Algorithm

```{r Naive, echo=FALSE, include=TRUE}
# Set seed 1234 for reproducibility
set.seed(1234)
# Build the model with Class as target and all other variables as predictors
naive_model <- naiveBayes(Class ~ ., data = train, laplace=1)
#Prediction with training and test data
predictions <- predict(naive_model, newdata=test)
# Compute the AUC and AUCPR for the Naive Model
pred <- prediction(as.numeric(predictions), test$Class)

auc_naive <- performance(pred, "auc")
auc_p_naive <- performance(pred, 'sens', 'spec')
aucpr_p_naive <- performance(pred, "prec", "rec")

aucpr_naive <- pr.curve(
  scores.class0 = predictions[test$Class == 1], 
  scores.class1 = predictions[test$Class == 0],
  curve = T,  
  dg.compute = T
)

# Make the relative plot
plot(aucpr_naive)
plot(auc_p_naive, main=paste("AUC:", auc_naive@y.values[[1]]))
plot(aucpr_p_naive, main=paste("AUCPR:", aucpr_naive$auc.integral))


# Create a dataframe 'results' that contains all metrics 
# obtained by the trained models

results <- data.frame(
  Model = "Naive Bayes", 
  AUC = auc_naive@y.values[[1]],
  AUCPR = aucpr_naive$auc.integral
)

# Show results on a table

results %>% 
  kable() %>%
  kable_styling(
    bootstrap_options = 
      c("striped", "hover", "condensed", "responsive"),
      position = "center",
      font_size = 10,
      full_width = FALSE
)


```

\newpage

### KNN

A KNN Model with k=5 can achieve a significant improvement in respect to the previous models, as regard AUCPR of **0.58** at the expense of a little drop off AUC, that is **0.81**.

```{r KNN, echo=FALSE, include=TRUE}
# Set seed 1234 for reproducibility

set.seed(1234)

# Build a KNN Model with Class as Target and all other variables as predictors. k is set to 5

knn_model <- knn(train[,-30], test[,-30], train$Class, k=5, prob = TRUE)

# Compute the AUC and AUCPR for the KNN Model

pred <- prediction(
  as.numeric(as.character(knn_model)), as.numeric(as.character(test$Class))
)

auc_knn <- performance(pred, "auc")

auc_p_knn <- performance(pred, 'sens', 'spec')
aucpr_p_knn <- performance(pred, "prec", "rec")

aucpr_knn <- pr.curve(
  scores.class0 = knn_model[test$Class == 1], 
  scores.class1 = knn_model[test$Class == 0],
  curve = T,  
  dg.compute = T
)

# Make the relative plot

plot(aucpr_knn)
plot(auc_p_knn, main=paste("AUC:", auc_knn@y.values[[1]]))
plot(aucpr_p_knn, main=paste("AUCPR:", aucpr_knn$auc.integral))

# Adding the respective metrics to the results dataset

results <- results %>% add_row(
  Model = "K-Nearest Neighbors k=5", 
  AUC = auc_knn@y.values[[1]],
  AUCPR = aucpr_knn$auc.integral
)

# Show results on a table

results %>% niceKable

```

\newpage

### Random Forest

The ensemble methods are capable of a significant increase in performance. At the expense of another little drop off in terms of AUC (**0.9**) respect to the Naive Bayes model, there is a huge step forward in terms of AUCPR, that is **0.77**. This model doesn't reach the desidered performance (AUCPR \> 0.85), but it's close to it. As the plot and the table below suggest, there are few predictors like **V17**, **V12** and **V14** that are particularly useful for classifying a fraud.

```{r RandomForest, echo=FALSE, include=TRUE}
# Set seed 1234 for reproducibility

set.seed(1234)

# Build a Random Forest Model with Class as Target and all other
# variables as predictors. The number of trees is set to 500

rf_model <- randomForest(Class ~ ., data = train, ntree = 500)

# Get the feature importance

feature_imp_rf <- data.frame(importance(rf_model))

# Make predictions based on this model

predictions <- predict(rf_model, newdata=test)

# Compute the AUC and AUPCR

pred <- prediction(
  as.numeric(as.character(predictions)),                                 as.numeric(as.character(test$Class))
)

auc_val_rf <- performance(pred, "auc")

auc_plot_rf <- performance(pred, 'sens', 'spec')

aucpr_plot_rf <- performance(pred, "prec", "rec", curve = T,  dg.compute = T)

aucpr_val_rf <- pr.curve(scores.class0 = predictions[test$Class == 1], scores.class1 = predictions[test$Class == 0],curve = T,  dg.compute = T)

# make the relative plot

plot(auc_plot_rf, main=paste("AUC:", auc_val_rf@y.values[[1]]))
plot(aucpr_plot_rf, main=paste("AUCPR:", aucpr_val_rf$auc.integral))
plot(aucpr_val_rf)

# Adding the respective metrics to the results dataset

results <- results %>% add_row(
  Model = "Random Forest",
  AUC = auc_val_rf@y.values[[1]],
  AUCPR = aucpr_val_rf$auc.integral)

# Show results on a table

results %>% niceKable

# Show feature importance on a table

feature_imp_rf %>% niceKable
```

\newpage

### XGBoost

XGBoost are a top class model. It always stays on TOP5 (or wins them) in every competitions on Kaggle and in this case, its' very fast to train and its performance are awesome. With an AUC of **0.98** and an AUCPR of **0.86** it reach and overtake the desidered performance. As the previous model shown, **V17** and **V14** are still relevant to predict a fraud.

```{r XGBoost, echo=FALSE, include=TRUE}
# Set seet 1234 for reproducibility

set.seed(1234)

# Prepare the training dataset

xgb_train <- xgb.DMatrix(
  as.matrix(train[, colnames(train) != "Class"]), 
  label = as.numeric(as.character(train$Class))
)

# Prepare the test dataset

xgb_test <- xgb.DMatrix(
  as.matrix(test[, colnames(test) != "Class"]), 
  label = as.numeric(as.character(test$Class))
)

# Prepare the cv dataset

xgb_cv <- xgb.DMatrix(
  as.matrix(cv[, colnames(cv) != "Class"]), 
  label = as.numeric(as.character(cv$Class))
)

# Prepare the parameters list. 

xgb_params <- list(
  objective = "binary:logistic", 
  eta = 0.1, 
  max.depth = 3, 
  nthread = 6, 
  eval_metric = "aucpr"
)

# Train the XGBoost Model

xgb_model <- xgb.train(
  data = xgb_train, 
  params = xgb_params, 
  watchlist = list(test = xgb_test, cv = xgb_cv), 
  nrounds = 500, 
  early_stopping_rounds = 40,
  verbosity = 0,
  print_every_n = 100,
  silent = T,

)

# Get feature importance
feature_imp_xgb <- xgb.importance(model = xgb_model)
xgb.plot.importance(feature_imp_xgb, rel_to_first = TRUE, xlab = "Relative importance")

# Make predictions based on this model

predictions = predict(
  xgb_model, 
  newdata = as.matrix(test[, colnames(test) != "Class"]), 
  ntreelimit = xgb_model$bestInd
)

# Compute the AUC and AUPCR

pred <- prediction(
  as.numeric(as.character(predictions)),                                 as.numeric(as.character(test$Class))
)

auc_val_xgb <- performance(pred, "auc")

auc_plot_xgb <- performance(pred, 'sens', 'spec')
aucpr_plot_xgb <- performance(pred, "prec", "rec")

aucpr_val_xgb <- pr.curve(
  scores.class0 = predictions[test$Class == 1], 
  scores.class1 = predictions[test$Class == 0],
  curve = T,  
  dg.compute = T
)

# Make the relative plot

plot(auc_plot_xgb, main=paste("AUC:", auc_val_xgb@y.values[[1]]))
plot(aucpr_plot_xgb, main=paste("AUCPR:", aucpr_val_xgb$auc.integral))
plot(aucpr_val_xgb)

# Adding the respective metrics to the results dataset

results <- results %>% add_row(
  Model = "XGBoost",
  AUC = auc_val_xgb@y.values[[1]],
  AUCPR = aucpr_val_xgb$auc.integral)

# Show results on a table

results %>% niceKable

# Show feature importance on a table

feature_imp_xgb %>% niceKable

```

\newpage

# Results

This is the summary results for all the models builted, trained and validated.

```{r Results, echo=FALSE, include=TRUE}
# Shows the results

results %>% niceKable
```

\newpage
# Conclusion

The ensemble methods once again confirm themselves as among the best models out there. It easy to find them as a winners of numerous Kaggle's competitions or on TOP5 of them. In this task, a XGBoost model can achieve a very good AUCPR result of **0.86** and the others ensembe methods are very close to it. As the features importance plots and table show, there are few predictors like **V17** and **V14** that are particularly useful for classifying a fraud. The SMOTE technique (a technique for dealing with imbalanced data) could improve the performance a little bit.

\newpage

# Literature

1.  [Rafael A. Irizarry, Introduction to Data Science](https://rafalab.github.io/dsbook/)
2.  [HarvardX Data Science Program](https://courses.edx.org/dashboard/programs/3c32e3e0-b6fe-4ee4-bd4f-210c6339e074/)
